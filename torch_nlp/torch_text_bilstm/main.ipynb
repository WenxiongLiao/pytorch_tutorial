{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#loading the data\n",
    "pd.read_csv('data/train.csv').head()\n",
    "\n",
    "from torchtext.data import Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-81a18813bf9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"valid.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             fields=trvld_datafields)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtst_datafields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"comment_text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\softwareinstall\\anaconda3\\lib\\site-packages\\torchtext\\data\\dataset.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         train_data = None if train is None else cls(\n\u001b[0;32m---> 78\u001b[0;31m             os.path.join(path, train), **kwargs)\n\u001b[0m\u001b[1;32m     79\u001b[0m         val_data = None if validation is None else cls(\n\u001b[1;32m     80\u001b[0m             os.path.join(path, validation), **kwargs)\n",
      "\u001b[0;32mC:\\softwareinstall\\anaconda3\\lib\\site-packages\\torchtext\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\softwareinstall\\anaconda3\\lib\\site-packages\\torchtext\\utils.py\u001b[0m in \u001b[0;36municode_csv_reader\u001b[0;34m(unicode_csv_data, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mmaxInt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxInt\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfield_size_limit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "#lowercased, whitespace-tokenized, and preprocessed\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "#labels is numeric already\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "from torchtext.data import TabularDataset\n",
    "trvld_datafields = [(\"id\", None),\n",
    "                (\"comment_text\", TEXT), (\"toxic\", LABEL),\n",
    "                (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n",
    "                (\"obscene\", LABEL), (\"insult\", LABEL),\n",
    "                (\"identity_hate\", LABEL)]\n",
    "\n",
    "trn, vld = TabularDataset.splits(\n",
    "            path=\"data\",\n",
    "            train=\"train.csv\", validation=\"valid.csv\",\n",
    "            format=\"csv\", skip_header=True,\n",
    "            fields=trvld_datafields)\n",
    "\n",
    "tst_datafields = [(\"id\", None), (\"comment_text\", TEXT)]\n",
    "tst = TabularDataset(\n",
    "        path=\"data/test.csv\", format=\"csv\", skip_header=True, fields=tst_datafields)\n",
    "\n",
    "print ('size of the train, dev, test dataset:', len(trn), len(vld), len(tst))\n",
    "#print (trn.fields.items())\n",
    "#print (trn[0].comment_text)\n",
    "#print (tst[0].comment_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "vec = vocab.Vectors('E:\\\\语料\\word2vec\\\\glove.6B\\\\glove.6B.100d.txt', '.\\\\data\\\\')\n",
    "TEXT.build_vocab(trn, vld, max_size=200, vectors=vec)\n",
    "print ('size of the vocab and embedding dim:', TEXT.vocab.vectors.shape) #size=202,50 (max vocab size=200+2 for <unk> and <pad>, and glove vector dim=50)\n",
    "print ('index of the in the vocab:', TEXT.vocab.stoi['the']) #output:2, so the index 2 in vocab is for 'the'\n",
    "print ('index 0 in the vocab:', TEXT.vocab.itos[0]) #output:<unk>, so the index 0 in vocab is for '<unk>'\n",
    "#print ('embedding vector for 'the':', TEXT.vocab.vectors[TEXT.vocab.stoi['the']])\n",
    "\n",
    "#print (TEXT.vocab.freqs.most_common(10))\n",
    "#print (trn[0].__dict__.keys())\n",
    "#print (trn[0].comment_text[:5])\n",
    "\n",
    "#BucketIterator groups sequence of similar lengths text in a batch together to minimize padding! how cool is that!\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter, val_iter = BucketIterator.splits((trn, vld),\n",
    "                                            batch_sizes=(3,3),\n",
    "                                            device='cuda',\n",
    "                                            sort_key=lambda x: len(x.comment_text), #tell the bucketIterator how to group the sequences\n",
    "                                            sort_within_batch=False,\n",
    "                                            repeat=False) #we want to wrap this Iterator layer\n",
    "test_iter = Iterator(tst, batch_size=3, device='cuda', sort=False, sort_within_batch=False, repeat=False)\n",
    "\n",
    "print ('number of batch (size: 3):', len(train_iter), len(val_iter)) #output: 9,9. because our train and val data only has 25 examples. since the batch size is 3, it means we have 25/3: 9 batches for each train and val\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print ('details of batch:', batch)\n",
    "print (\"the content of 'toxic' for the first 3 examples in batch 1:\", batch.toxic)\n",
    "#print (\"the content of 'comment_text' for the first 3 examples in batch 1, size depends on the longest sequence:\", batch.comment_text)\n",
    "#print (batch.dataset.fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x, y):\n",
    "        self.dl, self.x, self.y = dl, x, y\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x) #assuming one input\n",
    "            if self.y is not None: #concat the y into single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            yield (X,y)\n",
    "\n",
    "train_batch_it = BatchGenerator(train_iter, 'comment_text', ['toxic', 'threat'])\n",
    "#print ('get data x and y out of batch object:', next(iter(train_batch_it)))\n",
    "valid_batch_it = BatchGenerator(val_iter, 'comment_text', ['toxic', 'threat'])\n",
    "test_batch_it = BatchGenerator(test_iter, 'comment_text', None)\n",
    "#print (next(test_batch_it.__iter__()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from model import BiLSTM\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "emb_dim = 50\n",
    "hidden_dim = 50\n",
    "out_dim = 2 #only use 'toxic' and 'threat'\n",
    "pretrained_vec = trn.fields['comment_text'].vocab.vectors\n",
    "model = SimpleLSTM(vocab_size, hidden_dim, emb_dim, out_dim, pretrained_vec)\n",
    "print (model)\n",
    "model.cuda()\n",
    "\n",
    "import tqdm\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "epochs = 100\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    training_loss = 0.0\n",
    "    training_corrects = 0\n",
    "    model.train()\n",
    "    for x, y in tqdm.tqdm(train_batch_it):\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        training_loss += loss.item() * x.size(0)\n",
    "    epoch_loss = training_loss/ len(trn)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for x,y in valid_batch_it:\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "    val_loss /= len(vld)\n",
    "    train_loss.append(epoch_loss)\n",
    "    valid_loss.append(val_loss)\n",
    "    print ('Epoch: {}, Training loss: {:.4f}, Validation loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n",
    "    \n",
    "#predictions. note that the preds is the probability of the comment belong in each category output\n",
    "test_preds = []\n",
    "for x, y in tqdm.tqdm(test_batch_it):\n",
    "    preds = model(x)\n",
    "    preds = preds.data.cpu().numpy()\n",
    "    preds = 1/(1+np.exp(-preds)) #actual output of the model are logits, so we need to pass into sigmoid function\n",
    "    test_preds.append(preds)\n",
    "    print (y, ' >>> ',  preds)\n",
    "\n",
    "test_preds = np.hstack(test_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ep = range(1, epochs+1)\n",
    "print (ep)\n",
    "plt.plot(ep, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(ep, valid_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('plt.png')\n",
    "#print (test_preds)\n",
    "\n",
    "import sys\n",
    "sys.exit()\n",
    "\n",
    "#write predictions to csv\n",
    "df = pd.read_csv(\"data/test.csv\") \n",
    "for i, col in enumerate([\"toxic\", \"threat\"]):\n",
    "    df[col] = test_preds[:, i]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/littleflow3r/bilstm-pytorch-torchtext"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
